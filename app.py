
import streamlit as st
import pandas as pd
from io import BytesIO
from datetime import datetime
import re, ast, unicodedata

# ---------------------------
# ğŸ”„ Restart helpers
# ---------------------------
def _restart_app():
    """Clear caches & widget states (including file_uploader) and rerun."""
    # rotate uploader key so the file_uploader fully resets
    st.session_state["uploader_key"] = st.session_state.get("uploader_key", 0) + 1
    # clear any previous uploader widget state keys
    for k in list(st.session_state.keys()):
        if str(k).startswith("uploader_"):
            del st.session_state[k]
    # clear caches
    try:
        st.cache_data.clear()
    except Exception:
        pass
    try:
        st.cache_resource.clear()
    except Exception:
        pass
    st.rerun()

st.set_page_config(page_title="ğŸ“Š Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ & ğŸ§© Î£Ï€Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Î¦Î¹Î»Î¯ÎµÏ‚", page_icon="ğŸ§©", layout="wide")
st.title("ğŸ“Š Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬")

# Ensure a stable uploader-key in session
if "uploader_key" not in st.session_state:
    st.session_state["uploader_key"] = 0

# ---------------------------
# Sidebar: Legal / Terms + Restart
# ---------------------------
with st.sidebar:
    # ğŸ”„ Restart button (always visible)
    if st.button("ğŸ”„ Î•Ï€Î±Î½ÎµÎºÎºÎ¯Î½Î·ÏƒÎ· ÎµÏ†Î±ÏÎ¼Î¿Î³Î®Ï‚", help="ÎšÎ±Î¸Î±ÏÎ¯Î¶ÎµÎ¹ Î¼Î½Î®Î¼Î·/Ï†Î¿ÏÏ„ÏÏƒÎµÎ¹Ï‚ ÎºÎ±Î¹ Î¾ÎµÎºÎ¹Î½Î¬ Î±Ï€ÏŒ Ï„Î·Î½ Î±ÏÏ‡Î®"):
        _restart_app()

    st.markdown("### âš–ï¸ ÎŒÏÎ¿Î¹ Ï‡ÏÎ®ÏƒÎ·Ï‚")
    terms_ok = st.checkbox("Î‘Ï€Î¿Î´Î­Ï‡Î¿Î¼Î±Î¹ Ï„Î¿Ï…Ï‚ ÏŒÏÎ¿Ï…Ï‚ Ï‡ÏÎ®ÏƒÎ·Ï‚", value=True)
    st.markdown("Â© 2025 â€¢ Î Î½ÎµÏ…Î¼Î±Ï„Î¹ÎºÎ¬ Î´Î¹ÎºÎ±Î¹ÏÎ¼Î±Ï„Î± â€¢ **Î Î±Î½Î±Î³Î¹ÏÏ„Î± Î“Î¹Î±Î½Î½Î¯Ï„ÏƒÎ±ÏÎ¿Ï…**")

with st.sidebar.expander("ÎšÎ¬Ï„Î¿Ï‡Î¿Ï‚/Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÏŒÏ‚ & Î†Î´ÎµÎ¹Î±", expanded=False):
    st.markdown("""
**ÎšÎ¬Ï„Î¿Ï‡Î¿Ï‚/Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÏŒÏ‚:** Î Î±Î½Î±Î³Î¹ÏÏ„Î± Î“Î¹Î±Î½Î½Î¯Ï„ÏƒÎ±ÏÎ¿Ï…  
**Î ÏÎ¿ÏŠÏŒÎ½:** Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬/ÎšÎ±Ï„Î±Î½Î¿Î¼Î® ÎœÎ±Î¸Î·Ï„ÏÎ½ Î‘Î„ Î”Î·Î¼Î¿Ï„Î¹ÎºÎ¿Ï  

- Î— ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Ï€ÏÎ¿Î¿ÏÎ¯Î¶ÎµÏ„Î±Î¹ Î±Ï€Î¿ÎºÎ»ÎµÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î³Î¹Î± **ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÎ® Ï‡ÏÎ®ÏƒÎ·** Î±Ï€ÏŒ ÏƒÏ‡Î¿Î»Î¹ÎºÎ­Ï‚ Î¼Î¿Î½Î¬Î´ÎµÏ‚/ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÎ¿ÏÏ‚.  
- **Î Î½ÎµÏ…Î¼Î±Ï„Î¹ÎºÎ¬ Î´Î¹ÎºÎ±Î¹ÏÎ¼Î±Ï„Î±:** Â© 2025 Î Î±Î½Î±Î³Î¹ÏÏ„Î± Î“Î¹Î±Î½Î½Î¯Ï„ÏƒÎ±ÏÎ¿Ï…. **Î‘Ï€Î±Î³Î¿ÏÎµÏÎµÏ„Î±Î¹** Î±Î½Ï„Î¹Î³ÏÎ±Ï†Î®, Î±Î½Î±Î´Î·Î¼Î¿ÏƒÎ¯ÎµÏ…ÏƒÎ· Î® Ï„ÏÎ¿Ï€Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï‡Ï‰ÏÎ¯Ï‚ **Î­Î³Î³ÏÎ±Ï†Î· Î¬Î´ÎµÎ¹Î±**.  
- **ÎœÎ· ÎµÎ¼Ï€Î¿ÏÎ¹ÎºÎ® Ï‡ÏÎ®ÏƒÎ·** ÎµÏ€Î¹Ï„ÏÎ­Ï€ÎµÏ„Î±Î¹ ÏƒÎµ ÏƒÏ‡Î¿Î»ÎµÎ¯Î± Î³Î¹Î± ÎµÏƒÏ‰Ï„ÎµÏÎ¹ÎºÎ® Î¿ÏÎ³Î¬Î½Ï‰ÏƒÎ·.  
- Î Î±ÏÎ­Ï‡ÎµÏ„Î±Î¹ â€œ**Ï‰Ï‚ Î­Ï‡ÎµÎ¹**â€ Ï‡Ï‰ÏÎ¯Ï‚ ÎµÎ³Î³Ï…Î®ÏƒÎµÎ¹Ï‚. Î¤Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î­Ï‡Î¿Ï…Î½ **Î²Î¿Î·Î¸Î·Ï„Î¹ÎºÏŒ** Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÎ± ÎºÎ±Î¹ **Î´ÎµÎ½ Ï…Ï€Î¿ÎºÎ±Î¸Î¹ÏƒÏ„Î¿ÏÎ½** ÎºÎ±Î½Î¿Î½Î¹ÏƒÏ„Î¹ÎºÎ­Ï‚ Î±Ï€Î¿Ï†Î¬ÏƒÎµÎ¹Ï‚ Î® Ï€Î±Î¹Î´Î±Î³Ï‰Î³Î¹ÎºÎ® ÎºÏÎ¯ÏƒÎ·.  
- **Î•Ï€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î±:** panayiotayiannitsarou@gmail.com
""")

with st.sidebar.expander("ğŸ”’ Î ÏÎ¿ÏƒÏ„Î±ÏƒÎ¯Î± Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ (GDPR â€“ ÎšÏÏ€ÏÎ¿Ï‚)", expanded=False):
    st.markdown("""
- Î¤Î± Î±ÏÏ‡ÎµÎ¯Î± Excel Î±Î½ÎµÎ²Î±Î¯Î½Î¿Ï…Î½ Î±Ï€ÏŒ Ï„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î· ÎºÎ±Î¹ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ½Ï„Î±Î¹ **Î¼ÏŒÎ½Î¿** Î³Î¹Î± Î¬Î¼ÎµÏƒÎ¿ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒ. Î— ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Î´ÎµÎ½ Î±Ï€Î¿Î¸Î·ÎºÎµÏÎµÎ¹ Î¼ÏŒÎ½Î¹Î¼Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±.  
- ÎŸ Ï‡ÏÎ®ÏƒÏ„Î·Ï‚/ÏƒÏ‡Î¿Î»ÎµÎ¯Î¿ ÎµÏ…Î¸ÏÎ½ÎµÏ„Î±Î¹ Î³Î¹Î± ÏƒÏ…Î¼Î¼ÏŒÏÏ†Ï‰ÏƒÎ· Î¼Îµ **GDPR**.  
- **Î£Ï…ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚:** ÏˆÎµÏ…Î´ÏÎ½Ï…Î¼Î±/ÎºÏ‰Î´Î¹ÎºÎ¿Î¯, ÎµÎ»Î±Ï‡Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½, Ï€ÎµÏÎ¯Î¿Î´Î¿Ï‚ Î´Î¹Î±Ï„Î®ÏÎ·ÏƒÎ·Ï‚, ÎµÎ½Î·Î¼Î­ÏÏ‰ÏƒÎ· DPO, Î­Î»ÎµÎ³Ï‡Î¿Ï‚ Ï€Î±ÏÏŒÏ‡Î¿Ï… cloud.
""")

if not terms_ok:
    st.warning("âš ï¸ Î“Î¹Î± Î½Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹Ï‚ Ï„Î·Î½ ÎµÏ†Î±ÏÎ¼Î¿Î³Î®, Î±Ï€Î¿Î´Î­Î¾Î¿Ï… Ï„Î¿Ï…Ï‚ ÏŒÏÎ¿Ï…Ï‚ Ï‡ÏÎ®ÏƒÎ·Ï‚ (Î±ÏÎ¹ÏƒÏ„ÎµÏÎ¬).")
    st.stop()

with st.expander("ğŸ“œ Î Î»Î®ÏÎµÎ¹Ï‚ ÎŒÏÎ¿Î¹ Î§ÏÎ®ÏƒÎ·Ï‚ & Î‘Ï€Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î•Ï…Î¸ÏÎ½Î·Ï‚", expanded=False):
    st.markdown("""
1) **Î£ÎºÎ¿Ï€ÏŒÏ‚:** Î¥Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î· ÎµÏƒÏ‰Ï„ÎµÏÎ¹ÎºÎ¿Ï Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÎ¼Î¿Ï/ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÏÎ½ Ï„Î¬Î¾ÎµÏ‰Î½ Î‘Î„ Î”Î·Î¼Î¿Ï„Î¹ÎºÎ¿Ï.  
2) **Î”ÎµÎ´Î¿Î¼Î­Î½Î±:** Î”ÎµÎ½ Î±Ï€Î¿Î¸Î·ÎºÎµÏÎ¿Î½Ï„Î±Î¹ Î¼ÏŒÎ½Î¹Î¼Î± Î±Ï€ÏŒ Ï„Î·Î½ ÎµÏ†Î±ÏÎ¼Î¿Î³Î®. ÎŸ Ï‡ÏÎ®ÏƒÏ„Î·Ï‚ Ï€Î±ÏÎ±Î¼Î­Î½ÎµÎ¹ Ï…Ï€ÎµÏÎ¸Ï…Î½Î¿Ï‚ Î³Î¹Î± **GDPR**.  
3) **Î ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼Î¿Î¯:** Î‘Ï€Î±Î³Î¿ÏÎµÏÎµÏ„Î±Î¹ ÎµÎ¼Ï€Î¿ÏÎ¹ÎºÎ® ÎµÎºÎ¼ÎµÏ„Î¬Î»Î»ÎµÏ…ÏƒÎ·/Î±Î½Î±Î´Î¹Î±Î½Î¿Î¼Î®/Ï„ÏÎ¿Ï€Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï‡Ï‰ÏÎ¯Ï‚ Î¬Î´ÎµÎ¹Î±.  
4) **Î‘Ï€Î¿Ï€Î¿Î¯Î·ÏƒÎ·:** Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÎµÏ…Î¸ÏÎ½Î· Î³Î¹Î± Î±Ï€Î¿Ï†Î¬ÏƒÎµÎ¹Ï‚ Ï€Î¿Ï… Î»Î±Î¼Î²Î¬Î½Î¿Î½Ï„Î±Î¹ Î±Ï€Î¿ÎºÎ»ÎµÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±.  
5) **Î¤ÏÎ¿Ï€Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹Ï‚:** Î— ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÎ½Î·Î¼ÎµÏÏÎ½ÎµÏ„Î±Î¹ Ï‡Ï‰ÏÎ¯Ï‚ Ï€ÏÎ¿ÎµÎ¹Î´Î¿Ï€Î¿Î¯Î·ÏƒÎ·.
""")

# ---------------------------
# Canonicalization / Renaming
# ---------------------------
def _canon(s: str) -> str:
    return "".join((s or "").replace("_"," ").split()).upper()

CANON_TARGETS = {
    "ÎŸÎÎŸÎœÎ‘": {"ÎŸÎÎŸÎœÎ‘"},
    "Î¦Î¥Î›ÎŸ": {"Î¦Î¥Î›ÎŸ"},
    "Î Î‘Î™Î”Î™_Î•ÎšÎ Î‘Î™Î”Î•Î¥Î¤Î™ÎšÎŸÎ¥": {"Î Î‘Î™Î”Î™Î•ÎšÎ Î‘Î™Î”Î•Î¥Î¤Î™ÎšÎŸÎ¥", "Î Î‘Î™Î”Î™-Î•ÎšÎ Î‘Î™Î”Î•Î¥Î¤Î™ÎšÎŸÎ¥"},
    "Î–Î©Î—Î¡ÎŸÎ£": {"Î–Î©Î—Î¡ÎŸÎ£"},
    "Î™Î”Î™Î‘Î™Î¤Î•Î¡ÎŸÎ¤Î—Î¤Î‘": {"Î™Î”Î™Î‘Î™Î¤Î•Î¡ÎŸÎ¤Î—Î¤Î‘"},
    "ÎšÎ‘Î›Î—_Î“ÎÎ©Î£Î—_Î•Î›Î›Î—ÎÎ™ÎšÎ©Î": {"ÎšÎ‘Î›Î—Î“ÎÎ©Î£Î—Î•Î›Î›Î—ÎÎ™ÎšÎ©Î", "Î“ÎÎ©Î£Î—Î•Î›Î›Î—ÎÎ™ÎšÎ©Î"},
    "Î¦Î™Î›ÎŸÎ™": {"Î¦Î™Î›ÎŸÎ™", "Î¦Î™Î›Î™Î‘", "Î¦Î™Î›ÎŸÎ£"},
    "Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—": {"Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—", "Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î•Î™Î£"},
    "Î¤ÎœÎ—ÎœÎ‘": {"Î¤ÎœÎ—ÎœÎ‘"},
}
REQUIRED_COLS = ["ÎŸÎÎŸÎœÎ‘","Î¦Î¥Î›ÎŸ","Î Î‘Î™Î”Î™_Î•ÎšÎ Î‘Î™Î”Î•Î¥Î¤Î™ÎšÎŸÎ¥","Î–Î©Î—Î¡ÎŸÎ£","Î™Î”Î™Î‘Î™Î¤Î•Î¡ÎŸÎ¤Î—Î¤Î‘","ÎšÎ‘Î›Î—_Î“ÎÎ©Î£Î—_Î•Î›Î›Î—ÎÎ™ÎšÎ©Î","Î¦Î™Î›ÎŸÎ™","Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—","Î¤ÎœÎ—ÎœÎ‘"]

def auto_rename_columns(df: pd.DataFrame):
    """
    Rename common Greek columns to a canonical set.
    More permissive for 'Î¦Î™Î›ÎŸÎ™': detects any column whose canonical contains 'Î¦Î™Î›' or 'FRIEND'.
    If multiple friend-like columns exist, they are concatenated into one.
    """
    mapping = {}
    seen = set()
    for col in df.columns:
        c = _canon(col)
        found_target = None
        for target, keys in CANON_TARGETS.items():
            if c in keys and target not in seen:
                found_target = target
                seen.add(target)
                break
        if found_target:
            mapping[col] = found_target

    renamed = df.rename(columns=mapping)

    # Fallbacks for friends column if not mapped already
    friends_cols = [c for c in renamed.columns if c in ("Î¦Î™Î›ÎŸÎ™","Î¦Î™Î›Î™Î‘","Î¦Î™Î›ÎŸÎ£")]
    if not friends_cols:
        candidates = []
        for col in df.columns:
            c = _canon(col)
            if "Î¦Î™Î›" in c or "FRIEND" in c:
                candidates.append(col)
        if candidates:
            combined = []
            for _, row in df[candidates].astype(str).iterrows():
                vals = [str(v).strip() for v in row.tolist() if str(v).strip() and str(v).strip().upper() not in ("-", "NA", "NAN")]
                combined.append(", ".join(vals))
            renamed["Î¦Î™Î›ÎŸÎ™"] = combined

    # Ensure Î¤ÎœÎ—ÎœÎ‘ exists: if not, try to pick the rightmost column with short labels
    if "Î¤ÎœÎ—ÎœÎ‘" not in renamed.columns:
        best = None
        for col in df.columns[::-1]:
            s = df[col].dropna().astype(str).str.strip()
            if not len(s):
                continue
            if s.str.len().median() <= 4 and s.nunique() <= 10:
                best = col
                break
        if best:
            renamed = renamed.rename(columns={best: "Î¤ÎœÎ—ÎœÎ‘"})


    # âœ… Ensure 'Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—' column always exists (normalize plural -> singular or create empty)
    if "Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—" not in renamed.columns:
        if "Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î•Î™Î£" in renamed.columns:
            renamed = renamed.rename(columns={"Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î•Î™Î£": "Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—"})
        else:
            renamed["Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—"] = ""
    return renamed, mapping

# âœ… Backward-compat alias to avoid NameError if any old code references the typo
def auto_ename_columns(*args, **kwargs):
    return auto_rename_columns(*args, **kwargs)

def _normalize_yes_no(series: pd.Series) -> pd.Series:
    if series.dtype == object:
        s = series.fillna("").astype(str).str.strip().str.upper()
        s = s.replace({
            "ÎÎ‘Î™":"Î","NAI":"Î","YES":"Î","Y":"Î",
            "ÎŸÎ§Î™":"ÎŸ","OXI":"ÎŸ","NO":"ÎŸ","N":"ÎŸ","": "ÎŸ"
        })
        return s.where(s.isin(["Î","ÎŸ"]), other="ÎŸ")
    return series

# ---------------------------
# Broken friendships helpers
# ---------------------------
def _strip_diacritics(s: str) -> str:
    nfkd = unicodedata.normalize("NFD", s)
    return "".join(ch for ch in nfkd if not unicodedata.combining(ch))

def _canon_name(s: str) -> str:
    s = (str(s) if s is not None else "").strip()
    s = s.strip("[]'\" ")
    s = re.sub(r"\s+", " ", s)
    s = _strip_diacritics(s).upper()
    return s

_SPLIT_RE = re.compile(r"\s*(?:,|;|/|\||\band\b|\bÎºÎ±Î¹\b|\+|\n)\s*", flags=re.IGNORECASE)

def _parse_friends(cell):
    raw = str(cell) if cell is not None else ""
    raw = raw.strip()
    if not raw:
        return []
    if raw.startswith("[") and raw.endswith("]"):
        try:
            val = ast.literal_eval(raw)
            if isinstance(val, (list, tuple)):
                return [_canon_name(x) for x in val if str(x).strip()]
        except Exception:
            pass
        raw2 = raw.strip("[]")
        parts = re.split(r"[;,]", raw2)
        return [_canon_name(p) for p in parts if _canon_name(p)]
    parts = [p for p in _SPLIT_RE.split(raw) if p]
    return [_canon_name(p) for p in parts if _canon_name(p)]

def list_broken_mutual_pairs(df: pd.DataFrame) -> pd.DataFrame:
    """Return DataFrame with each broken mutual pair (A/B with classes)."""
    fcol = None
    for candidate in ["Î¦Î™Î›ÎŸÎ™","Î¦Î™Î›Î™Î‘","Î¦Î™Î›ÎŸÎ£"]:
        if candidate in df.columns:
            fcol = candidate
            break
    if fcol is None or "ÎŸÎÎŸÎœÎ‘" not in df.columns or "Î¤ÎœÎ—ÎœÎ‘" not in df.columns:
        return pd.DataFrame(columns=["A","A_Î¤ÎœÎ—ÎœÎ‘","B","B_Î¤ÎœÎ—ÎœÎ‘"])

    df = df.copy()
    df["__CAN_NAME__"] = df["ÎŸÎÎŸÎœÎ‘"].map(_canon_name)
    name_to_original = dict(zip(df["__CAN_NAME__"], df["ÎŸÎÎŸÎœÎ‘"].astype(str)))
    class_by_name = dict(zip(df["__CAN_NAME__"], df["Î¤ÎœÎ—ÎœÎ‘"].astype(str).str.strip()))

    token_index = {}
    for full in df["__CAN_NAME__"]:
        tokens = [t for t in re.split(r"\s+", full) if t]
        for t in tokens:
            token_index.setdefault(t, set()).add(full)

    def resolve_friend(s: str):
        s = _canon_name(s)
        if not s:
            return None
        if s in name_to_original:
            return s
        toks = [t for t in re.split(r"\s+", s) if t]
        if not toks:
            return None
        if len(toks) >= 2:
            sets = [token_index.get(t, set()) for t in toks]
            inter = set.intersection(*sets) if sets else set()
            if len(inter) == 1:
                return next(iter(inter))
            union = set().union(*sets)
            if len(union) == 1:
                return next(iter(union))
            return None
        else:
            group = token_index.get(toks[0], set())
            return next(iter(group)) if len(group) == 1 else None

    friends_by_name = {}
    for _, row in df.iterrows():
        me = row["__CAN_NAME__"]
        flist_raw = _parse_friends(row[fcol])
        resolved = set()
        for fr in flist_raw:
            r = resolve_friend(fr)
            if r and r != me:
                resolved.add(r)
        friends_by_name[me] = resolved

    mutual_pairs = set()
    for a, flist in friends_by_name.items():
        for b in flist:
            if b in friends_by_name and a in friends_by_name[b]:
                mutual_pairs.add(tuple(sorted([a,b])))

    rows = []
    for a, b in sorted(mutual_pairs):
        ta = class_by_name.get(a, "")
        tb = class_by_name.get(b, "")
        if ta and tb and ta != tb:
            rows.append({
                "A": name_to_original.get(a, a), "A_Î¤ÎœÎ—ÎœÎ‘": ta,
                "B": name_to_original.get(b, b), "B_Î¤ÎœÎ—ÎœÎ‘": tb,
            })
    return pd.DataFrame(rows)

def friend_resolution_diagnostics(df: pd.DataFrame):
    """Return (broken_df, diagnostics) where diagnostics includes unmatched and ambiguous tokens."""
    fcol = None
    for candidate in ["Î¦Î™Î›ÎŸÎ™","Î¦Î™Î›Î™Î‘","Î¦Î™Î›ÎŸÎ£"]:
        if candidate in df.columns:
            fcol = candidate
            break
    if fcol is None or "ÎŸÎÎŸÎœÎ‘" not in df.columns or "Î¤ÎœÎ—ÎœÎ‘" not in df.columns:
        return pd.DataFrame(columns=["A","A_Î¤ÎœÎ—ÎœÎ‘","B","B_Î¤ÎœÎ—ÎœÎ‘"]), {"unmatched": [], "ambiguous": {}}

    df = df.copy()
    df["__CAN_NAME__"] = df["ÎŸÎÎŸÎœÎ‘"].map(_canon_name)
    name_to_original = dict(zip(df["__CAN_NAME__"], df["ÎŸÎÎŸÎœÎ‘"].astype(str)))
    class_by_name = dict(zip(df["__CAN_NAME__"], df["Î¤ÎœÎ—ÎœÎ‘"].astype(str).str.strip()))

    token_index = {}
    for full in df["__CAN_NAME__"]:
        tokens = [t for t in re.split(r"\s+", full) if t]
        for t in tokens:
            token_index.setdefault(t, set()).add(full)

    def resolve_friend_diagnose(s: str):
        s = _canon_name(s)
        if not s:
            return None, None
        if s in name_to_original:
            return s, "exact"
        toks = [t for t in re.split(r"\s+", s) if t]
        if not toks:
            return None, None
        if len(toks) >= 2:
            sets = [token_index.get(t, set()) for t in toks]
            inter = set.intersection(*sets) if sets else set()
            if len(inter) == 1:
                return next(iter(inter)), "intersection"
            union = set().union(*sets)
            if len(union) == 1:
                return next(iter(union)), "union-unique"
            return None, ("ambiguous", list(sorted(union)))
        else:
            group = token_index.get(toks[0], set())
            if len(group) == 1:
                return next(iter(group)), "single-unique"
            elif len(group) > 1:
                return None, ("ambiguous", list(sorted(group)))
            else:
                return None, None

    friends_by_name = {}
    unmatched = set()
    ambiguous = {}
    for _, row in df.iterrows():
        me = row["__CAN_NAME__"]
        flist_raw = _parse_friends(row[fcol])
        resolved = set()
        for fr in flist_raw:
            r, how = resolve_friend_diagnose(fr)
            if r is None:
                if isinstance(how, tuple) and how[0] == "ambiguous":
                    ambiguous[_canon_name(fr)] = [name_to_original.get(x, x) for x in how[1]]
                else:
                    unmatched.add(_canon_name(fr))
            elif r != me:
                resolved.add(r)
        friends_by_name[me] = resolved

    mutual_pairs = set()
    for a, flist in friends_by_name.items():
        for b in flist:
            if b in friends_by_name and a in friends_by_name[b]:
                mutual_pairs.add(tuple(sorted([a,b])))

    rows = []
    for a, b in sorted(mutual_pairs):
        ta = class_by_name.get(a, "")
        tb = class_by_name.get(b, "")
        if ta and tb and ta != tb:
            rows.append({
                "A": name_to_original.get(a, a), "A_Î¤ÎœÎ—ÎœÎ‘": ta,
                "B": name_to_original.get(b, b), "B_Î¤ÎœÎ—ÎœÎ‘": tb,
            })
    broken_df = pd.DataFrame(rows)
    return broken_df, {"unmatched": sorted(unmatched), "ambiguous": ambiguous}

def broken_count_by_class(df: pd.DataFrame) -> pd.Series:
    pairs = list_broken_mutual_pairs(df)
    if pairs.empty:
        return pd.Series({tmima: 0 for tmima in df["Î¤ÎœÎ—ÎœÎ‘"].dropna().astype(str).str.strip().unique()})
    counts = {}
    for _, row in pairs.iterrows():
        a_c = str(row["A_Î¤ÎœÎ—ÎœÎ‘"]).strip()
        b_c = str(row["B_Î¤ÎœÎ—ÎœÎ‘"]).strip()
        counts[a_c] = counts.get(a_c, 0) + 1
        counts[b_c] = counts.get(b_c, 0) + 1
    return pd.Series(counts).astype(int)


# ---------------------------
# Conflicts helpers (Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—)
# ---------------------------
def _parse_conflict_targets(cell):
    """Parse Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î— cell to list of canonical names (supports comma/semicolon/slash/pipe/newline)."""
    raw = str(cell) if cell is not None else ""
    raw = raw.strip()
    if not raw:
        return []
    if raw.startswith("[") and raw.endswith("]"):
        try:
            val = ast.literal_eval(raw)
            if isinstance(val, (list, tuple)):
                return [_canon_name(x) for x in val if str(x).strip()]
        except Exception:
            pass
        raw2 = raw.strip("[]")
        parts = re.split(r"[;,]", raw2)
        return [_canon_name(p) for p in parts if _canon_name(p)]
    parts = [p for p in _SPLIT_RE.split(raw) if p]
    return [_canon_name(p) for p in parts if _canon_name(p)]

def _build_name_resolution(df: pd.DataFrame):
    df = df.copy()
    df["__CAN_NAME__"] = df["ÎŸÎÎŸÎœÎ‘"].map(_canon_name)
    name_to_original = dict(zip(df["__CAN_NAME__"], df["ÎŸÎÎŸÎœÎ‘"].astype(str)))
    class_by_name = dict(zip(df["__CAN_NAME__"], df["Î¤ÎœÎ—ÎœÎ‘"].astype(str).str.strip()))
    token_index = {}
    for full in df["__CAN_NAME__"]:
        tokens = [t for t in re.split(r"\s+", full) if t]
        for t in tokens:
            token_index.setdefault(t, set()).add(full)
    def resolve_name(s: str):
        s = _canon_name(s)
        if not s:
            return None
        if s in name_to_original:
            return s
        toks = [t for t in re.split(r"\s+", s) if t]
        if not toks:
            return None
        if len(toks) >= 2:
            sets = [token_index.get(t, set()) for t in toks]
            inter = set.intersection(*sets) if sets else set()
            if len(inter) == 1:
                return next(iter(inter))
            union = set().union(*sets)
            if len(union) == 1:
                return next(iter(union))
            return None
        else:
            group = token_index.get(toks[0], set())
            return next(iter(group)) if len(group) == 1 else None
    return name_to_original, class_by_name, resolve_name


def compute_conflict_counts_and_pairs(df: pd.DataFrame):
    """
    Return (counts_series, pairs_df, names_series).
    - counts_series: per-student integer count of conflicts seated in the SAME class.
    - pairs_df: deduplicated pairs (A,B) where either A listed B (or B listed A) and both are in the same class.
    - names_series: per-student comma-separated string of conflict names that are in the same class.
    """
    required = {"ÎŸÎÎŸÎœÎ‘", "Î¤ÎœÎ—ÎœÎ‘", "Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—"}
    if not required.issubset(set(df.columns)):
        return (
            pd.Series([0]*len(df), index=df.index),
            pd.DataFrame(columns=["A","A_Î¤ÎœÎ—ÎœÎ‘","B","B_Î¤ÎœÎ—ÎœÎ‘"]),
            pd.Series([""]*len(df), index=df.index),
        )

    name_to_original, class_by_name, resolve_name = _build_name_resolution(df)

    # Build canonical name per row for alignment
    canon_names = df["ÎŸÎÎŸÎœÎ‘"].map(_canon_name)
    counts = [0]*len(df)
    names = [""]*len(df)
    pairs = set()

    # map index by canonical name for alignment
    index_by_canon = {cn: i for i, cn in enumerate(canon_names)}

    for i, row in df.iterrows():
        me = _canon_name(row["ÎŸÎÎŸÎœÎ‘"])
        my_class = class_by_name.get(me, "")
        targets = _parse_conflict_targets(row["Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—"])
        same_class_names = []
        for t in targets:
            r = resolve_name(t)
            if r and r != me:
                if class_by_name.get(r, None) == my_class and my_class:
                    same_class_names.append(name_to_original.get(r, r))
                    pair = tuple(sorted([me, r]))
                    pairs.add(pair)
        counts[index_by_canon.get(me, i)] = len(same_class_names)
        names[index_by_canon.get(me, i)] = ", ".join(same_class_names)

    rows = []
    for a, b in sorted(pairs):
        ta = class_by_name.get(a, "")
        tb = class_by_name.get(b, "")
        if ta == tb and ta:
            rows.append({
                "A": name_to_original.get(a, a), "A_Î¤ÎœÎ—ÎœÎ‘": ta,
                "B": name_to_original.get(b, b), "B_Î¤ÎœÎ—ÎœÎ‘": tb,
            })
    pairs_df = pd.DataFrame(rows)
    return pd.Series(counts, index=df.index), pairs_df, pd.Series(names, index=df.index)


def generate_stats(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if "Î¤ÎœÎ—ÎœÎ‘" in df:
        df["Î¤ÎœÎ—ÎœÎ‘"] = df["Î¤ÎœÎ—ÎœÎ‘"].apply(lambda v: v.strip() if isinstance(v, str) else v)
    if "Î¦Î¥Î›ÎŸ" in df:
        df["Î¦Î¥Î›ÎŸ"] = df["Î¦Î¥Î›ÎŸ"].fillna("").astype(str).str.strip().str.upper()
    for col in ["Î Î‘Î™Î”Î™_Î•ÎšÎ Î‘Î™Î”Î•Î¥Î¤Î™ÎšÎŸÎ¥","Î–Î©Î—Î¡ÎŸÎ£","Î™Î”Î™Î‘Î™Î¤Î•Î¡ÎŸÎ¤Î—Î¤Î‘","ÎšÎ‘Î›Î—_Î“ÎÎ©Î£Î—_Î•Î›Î›Î—ÎÎ™ÎšÎ©Î"]:
        if col in df:
            df[col] = _normalize_yes_no(df[col])

    boys = df[df["Î¦Î¥Î›ÎŸ"] == "Î‘"].groupby("Î¤ÎœÎ—ÎœÎ‘").size() if "Î¦Î¥Î›ÎŸ" in df else pd.Series(dtype=int)
    girls = df[df["Î¦Î¥Î›ÎŸ"] == "Îš"].groupby("Î¤ÎœÎ—ÎœÎ‘").size() if "Î¦Î¥Î›ÎŸ" in df else pd.Series(dtype=int)
    educators = df[df["Î Î‘Î™Î”Î™_Î•ÎšÎ Î‘Î™Î”Î•Î¥Î¤Î™ÎšÎŸÎ¥"] == "Î"].groupby("Î¤ÎœÎ—ÎœÎ‘").size() if "Î Î‘Î™Î”Î™_Î•ÎšÎ Î‘Î™Î”Î•Î¥Î¤Î™ÎšÎŸÎ¥" in df else pd.Series(dtype=int)
    energetic = df[df["Î–Î©Î—Î¡ÎŸÎ£"] == "Î"].groupby("Î¤ÎœÎ—ÎœÎ‘").size() if "Î–Î©Î—Î¡ÎŸÎ£" in df else pd.Series(dtype=int)
    special = df[df["Î™Î”Î™Î‘Î™Î¤Î•Î¡ÎŸÎ¤Î—Î¤Î‘"] == "Î"].groupby("Î¤ÎœÎ—ÎœÎ‘").size() if "Î™Î”Î™Î‘Î™Î¤Î•Î¡ÎŸÎ¤Î—Î¤Î‘" in df else pd.Series(dtype=int)
    greek = df[df["ÎšÎ‘Î›Î—_Î“ÎÎ©Î£Î—_Î•Î›Î›Î—ÎÎ™ÎšÎ©Î"] == "Î"].groupby("Î¤ÎœÎ—ÎœÎ‘").size() if "ÎšÎ‘Î›Î—_Î“ÎÎ©Î£Î—_Î•Î›Î›Î—ÎÎ™ÎšÎ©Î" in df else pd.Series(dtype=int)
    total = df.groupby("Î¤ÎœÎ—ÎœÎ‘").size() if "Î¤ÎœÎ—ÎœÎ‘" in df else pd.Series(dtype=int)
    broken = broken_count_by_class(df) if "Î¤ÎœÎ—ÎœÎ‘" in df else pd.Series(dtype=int)

    # âœ… Conflicts per class (count of conflict pairs seated in the same class)
    try:
        counts_series, pairs_df, _names_series = compute_conflict_counts_and_pairs(df)
        if pairs_df.empty:
            conflict_by_class = pd.Series({tmima: 0 for tmima in df["Î¤ÎœÎ—ÎœÎ‘"].dropna().astype(str).str.strip().unique()})
        else:
            conflict_counts = {}
            for _, row in pairs_df.iterrows():
                c = str(row["A_Î¤ÎœÎ—ÎœÎ‘"]).strip()
                conflict_counts[c] = conflict_counts.setdefault(c, 0) + 1
            conflict_by_class = pd.Series(conflict_counts).astype(int)
    except Exception:
        # Fallback safe default
        conflict_by_class = pd.Series({tmima: 0 for tmima in df.get("Î¤ÎœÎ—ÎœÎ‘", pd.Series(dtype=str)).dropna().astype(str).str.strip().unique()})

    stats = pd.DataFrame({
        "Î‘Î“ÎŸÎ¡Î™Î‘": boys,
        "ÎšÎŸÎ¡Î™Î¤Î£Î™Î‘": girls,
        "Î Î‘Î™Î”Î™_Î•ÎšÎ Î‘Î™Î”Î•Î¥Î¤Î™ÎšÎŸÎ¥": educators,
        "Î–Î©Î—Î¡ÎŸÎ™": energetic,
        "Î™Î”Î™Î‘Î™Î¤Î•Î¡ÎŸÎ¤Î—Î¤Î‘": special,
        "Î“ÎÎ©Î£Î— Î•Î›Î›Î—ÎÎ™ÎšÎ©Î": greek,
        "Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—": conflict_by_class,
        "Î£Î Î‘Î£ÎœÎ•ÎÎ— Î¦Î™Î›Î™Î‘": broken,
        "Î£Î¥ÎÎŸÎ›ÎŸ ÎœÎ‘Î˜Î—Î¤Î©Î": total,
    }).fillna(0).astype(int)

    if hasattr(stats.index, "str"):
        stats = stats.loc[stats.index.str.lower() != "nan"]
    try:
        stats = stats.sort_index(key=lambda x: x.str.extract(r"(\d+)")[0].astype(float))
    except Exception:
        stats = stats.sort_index()
    return stats

def export_stats_to_excel(stats_df: pd.DataFrame) -> BytesIO:
    output = BytesIO()
    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
        stats_df.to_excel(writer, index=True, sheet_name="Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬", index_label="Î¤ÎœÎ—ÎœÎ‘")
        wb = writer.book
        ws = writer.sheets["Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬"]
        header_fmt = wb.add_format({"bold": True, "valign":"vcenter", "text_wrap": True, "border":1})
        for col_idx, value in enumerate(["Î¤ÎœÎ—ÎœÎ‘"] + list(stats_df.columns)):
            ws.write(0, col_idx, value, header_fmt)
        for i in range(0, len(stats_df.columns)+1):
            ws.set_column(i, i, 18)
    output.seek(0)
    return output

def sanitize_sheet_name(s: str) -> str:
    s = str(s or "")
    s = re.sub(r'[:\\/?*\\[\\]]', ' ', s)
    return s[:31] if s else "SHEET"

def build_broken_report(xl: pd.ExcelFile, mode: str = "full") -> BytesIO:
    bio = BytesIO()
    summary_rows = []
    with pd.ExcelWriter(bio, engine="xlsxwriter") as writer:
        # copy originals
        for sheet in xl.sheet_names:
            df_raw = xl.parse(sheet_name=sheet)
            df_raw.to_excel(writer, index=False, sheet_name=sanitize_sheet_name(sheet))
        # broken per sheet
        for sheet in xl.sheet_names:
            df_raw = xl.parse(sheet_name=sheet)
            df_norm, _ = auto_rename_columns(df_raw)
            broken_df = list_broken_mutual_pairs(df_norm)
            summary_rows.append({"Î£ÎµÎ½Î¬ÏÎ¹Î¿ (sheet)": sheet, "Î£Ï€Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Î”Ï…Î¬Î´ÎµÏ‚": int(len(broken_df))})
            out_name = sanitize_sheet_name(f"{sheet}_BROKEN")
            if broken_df.empty:
                pd.DataFrame({"info": ["â€” ÎºÎ±Î¼Î¯Î± ÏƒÏ€Î±ÏƒÎ¼Î­Î½Î· â€”"]}).to_excel(writer, index=False, sheet_name=out_name)
            else:
                broken_df.to_excel(writer, index=False, sheet_name=out_name)
        # summary
        summary_df = pd.DataFrame(summary_rows).sort_values("Î£ÎµÎ½Î¬ÏÎ¹Î¿ (sheet)")
        summary_df.to_excel(writer, index=False, sheet_name="Î£ÏÎ½Î¿ÏˆÎ·")
    bio.seek(0)
    return bio

# ---------------------------
# Upload (with resettable key)
# ---------------------------
st.markdown("### ğŸ“¥ Î•Î¹ÏƒÎ±Î³Ï‰Î³Î® Î‘ÏÏ‡ÎµÎ¯Î¿Ï… Excel")
uploaded = st.file_uploader(
    "Î•Ï€Î¯Î»ÎµÎ¾Îµ **Excel** Î¼Îµ Î­Î½Î± Î® Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ± sheets (ÏƒÎµÎ½Î¬ÏÎ¹Î±)",
    type=["xlsx","xls"],
    key=f"uploader_{st.session_state['uploader_key']}"
)

if not uploaded:
    st.info("â• Î‘Î½Î­Î²Î±ÏƒÎµ Î­Î½Î± Excel Î³Î¹Î± Î½Î± ÏƒÏ…Î½ÎµÏ‡Î¯ÏƒÎµÎ¹Ï‚.")
    st.stop()

try:
    xl = pd.ExcelFile(uploaded)
    st.success(f"âœ… Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î±ÏÏ‡ÎµÎ¯Î¿Ï…: **{uploaded.name}** â€” Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(xl.sheet_names)} sheet(s).")
except Exception as e:
    st.error(f"âŒ Î£Ï†Î¬Î»Î¼Î± Î±Î½Î¬Î³Î½Ï‰ÏƒÎ·Ï‚: {e}")
    st.stop()

# ---------------------------
# Tabs
# ---------------------------
tab_stats, tab_broken = st.tabs(["ğŸ“Š Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ (1 sheet)", "ğŸ§© Î£Ï€Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Î±Î¼Î¿Î¹Î²Î±Î¯ÎµÏ‚ (ÏŒÎ»Î± Ï„Î± sheets) â€” ÎˆÎ¾Î¿Î´Î¿Ï‚: Î Î»Î®ÏÎµÏ‚ Î±Î½Ï„Î¯Î³ÏÎ±Ï†Î¿ + *_BROKEN + Î£ÏÎ½Î¿ÏˆÎ·"])

with tab_stats:
    st.subheader("ğŸ“Š Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÏÎ½ Î³Î¹Î± Î•Ï€Î¹Î»ÎµÎ³Î¼Î­Î½Î¿ Sheet")
    sheet = st.selectbox("Î”Î¹Î¬Î»ÎµÎ¾Îµ sheet", options=xl.sheet_names, index=0)
    df_raw = xl.parse(sheet_name=sheet)
    df_norm, ren_map = auto_rename_columns(df_raw)

    # âœ… Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Î¼ÎµÏ„ÏÎ·Ï„Î® Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—Î£ ÎºÎ±Î¹ Î¶ÎµÏ…Î³ÏÎ½ ÏƒÏ„Î·Î½ Î¯Î´Î¹Î± Ï„Î¬Î¾Î·
    conflict_counts, conflict_pairs, conflict_names = compute_conflict_counts_and_pairs(df_norm)
    try:
        df_with_conflicts = df_norm.copy()
        df_with_conflicts["Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—"] = conflict_counts.astype(int)
        df_with_conflicts["Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—_ÎŸÎÎŸÎœÎ‘"] = conflict_names
    except Exception:
        df_with_conflicts = df_norm

    missing = [c for c in REQUIRED_COLS if c not in df_norm.columns]
    with st.expander("ğŸ” Î”Î¹Î¬Î³Î½Ï‰ÏƒÎ·/ÎœÎµÏ„Î¿Î½Î¿Î¼Î±ÏƒÎ¯ÎµÏ‚", expanded=False):
        st.write("Î‘Î½Î±Î³Î½Ï‰ÏÎ¹ÏƒÎ¼Î­Î½ÎµÏ‚ ÏƒÏ„Î®Î»ÎµÏ‚:", list(df_norm.columns))
        if ren_map:
            st.write("Î‘Ï…Ï„ÏŒÎ¼Î±Ï„ÎµÏ‚ Î¼ÎµÏ„Î¿Î½Î¿Î¼Î±ÏƒÎ¯ÎµÏ‚:", ren_map)
        if missing:
            st.error("âŒ Î›ÎµÎ¯Ï€Î¿Ï…Î½ Ï…Ï€Î¿Ï‡ÏÎµÏ‰Ï„Î¹ÎºÎ­Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚: " + ", ".join(missing))

    if not missing:
        with st.expander("ğŸ‘ï¸ Î ÏÎ¿Î²Î¿Î»Î® Ï€Î¯Î½Î±ÎºÎ± Î¼Î±Î¸Î·Ï„ÏÎ½ Î¼Îµ Î¼ÎµÏ„ÏÎ·Ï„Î® Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—Î£ ÎºÎ±Î¹ Ï„Î¼Î®Î¼Î±", expanded=False):
            st.dataframe(df_with_conflicts, use_container_width=True)
        with st.expander("ğŸš« Î–ÎµÏÎ³Î· ÏƒÏÎ³ÎºÏÎ¿Ï…ÏƒÎ·Ï‚ Ï€Î¿Ï… Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÏƒÏ„Î·Î½ Î¯Î´Î¹Î± Ï„Î¬Î¾Î· (Î³Î¹Î± Ï„Î¿ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î¿ sheet)", expanded=False):
            if conflict_pairs.empty:
                st.info("â€” Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î¶ÎµÏÎ³Î· ÏƒÏÎ³ÎºÏÎ¿Ï…ÏƒÎ·Ï‚ ÏƒÏ„Î·Î½ Î¯Î´Î¹Î± Ï„Î¬Î¾Î· â€”")
            else:
                st.dataframe(conflict_pairs, use_container_width=True)


        # ğŸ§© Î£Ï€Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Î±Î¼Î¿Î¹Î²Î±Î¯ÎµÏ‚ Î³Î¹Î± Ï„Î¿ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î¿ sheet â€” ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ· Î¿Î½Î¿Î¼Î¬Ï„Ï‰Î½ (+ Ï†Î¯Î»Ï„ÏÎ¿ Î±Î½Î¬ Ï„Î¼Î®Î¼Î±)
        with st.expander("ğŸ§© Î£Ï€Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Î±Î¼Î¿Î¹Î²Î±Î¯ÎµÏ‚ (Î¿Î½ÏŒÎ¼Î±Ï„Î±) Î³Î¹Î± Ï„Î¿ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î¿ sheet", expanded=False):
            try:
                broken_df_for_sheet = list_broken_mutual_pairs(df_norm)
                if broken_df_for_sheet.empty:
                    st.info("â€” Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÏƒÏ€Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Ï€Î»Î®ÏÏ‰Ï‚ Î±Î¼Î¿Î¹Î²Î±Î¯ÎµÏ‚ Î´Ï…Î¬Î´ÎµÏ‚ ÏƒÏ„Î¿ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î¿ sheet â€”")
                else:
                    classes = sorted(set(broken_df_for_sheet["A_Î¤ÎœÎ—ÎœÎ‘"].astype(str)) | set(broken_df_for_sheet["B_Î¤ÎœÎ—ÎœÎ‘"].astype(str)))
                    sel = st.selectbox("Î¦Î¯Î»Ï„ÏÎ¿ Î±Î½Î¬ Ï„Î¼Î®Î¼Î±", options=["ÎŒÎ»Î±"] + classes, index=0)
                    if sel != "ÎŒÎ»Î±":
                        mask = (broken_df_for_sheet["A_Î¤ÎœÎ—ÎœÎ‘"].astype(str) == sel) | (broken_df_for_sheet["B_Î¤ÎœÎ—ÎœÎ‘"].astype(str) == sel)
                        view_df = broken_df_for_sheet[mask].reset_index(drop=True)
                    else:
                        view_df = broken_df_for_sheet.reset_index(drop=True)
                    st.dataframe(view_df, use_container_width=True)
                    # Download as Excel
                    from io import BytesIO
                    bio = BytesIO()
                    with pd.ExcelWriter(bio, engine="xlsxwriter") as writer:
                        view_df.to_excel(writer, index=False, sheet_name="Î£Ï€Î±ÏƒÎ¼Î­Î½ÎµÏ‚_Î”Ï…Î¬Î´ÎµÏ‚")
                    bio.seek(0)
                    st.download_button(
                        "â¬‡ï¸ ÎšÎ±Ï„Î­Î²Î±ÏƒÎµ Î¿Î½ÏŒÎ¼Î±Ï„Î± ÏƒÏ€Î±ÏƒÎ¼Î­Î½Ï‰Î½ Î´Ï…Î¬Î´Ï‰Î½ (Excel)",
                        data=bio.getvalue(),
                        file_name=f"broken_pairs_{sanitize_sheet_name(sheet)}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
            except Exception as e:
                st.warning(f"Î”ÎµÎ½ Î®Ï„Î±Î½ Î´Ï…Î½Î±Ï„Î® Î· ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ· Î¿Î½Î¿Î¼Î¬Ï„Ï‰Î½ ÏƒÏ€Î±ÏƒÎ¼Î­Î½Ï‰Î½ Î´Ï…Î¬Î´Ï‰Î½: {e}")


    if not missing:
        stats_df = generate_stats(df_norm)
        st.dataframe(stats_df, use_container_width=True)
        st.download_button(
            "ğŸ’¾ Î›Î®ÏˆÎ· Î Î¯Î½Î±ÎºÎ± Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÏÎ½ (Excel)",
            data=export_stats_to_excel(stats_df).getvalue(),
            file_name=f"statistika_{sanitize_sheet_name(sheet)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            type="primary"
        )
    else:
        st.info("Î£Ï…Î¼Ï€Î»Î®ÏÏ‰ÏƒÎµ/Î´Î¹ÏŒÏÎ¸Ï‰ÏƒÎµ Ï„Î¹Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚ Ï€Î¿Ï… Î»ÎµÎ¯Ï€Î¿Ï…Î½ ÏƒÏ„Î¿ Excel ÎºÎ±Î¹ Î¾Î±Î½Î±Ï†ÏŒÏÏ„Ï‰ÏƒÎ­ Ï„Î¿.")

with tab_broken:
    st.subheader("ğŸ§© Î‘Î½Î±Ï†Î¿ÏÎ¬ Î£Ï€Î±ÏƒÎ¼Î­Î½Ï‰Î½ Î Î»Î®ÏÏ‰Ï‚ Î‘Î¼Î¿Î¹Î²Î±Î¯Ï‰Î½ Î”Ï…Î¬Î´Ï‰Î½ (ÏŒÎ»Î± Ï„Î± sheets)")
    # Summary table
    summary_rows = []
    for sheet in xl.sheet_names:
        df_raw = xl.parse(sheet_name=sheet)
        df_norm, _ = auto_rename_columns(df_raw)
        broken_df = list_broken_mutual_pairs(df_norm)
        summary_rows.append({"Î£ÎµÎ½Î¬ÏÎ¹Î¿ (sheet)": sheet, "Î£Ï€Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Î”Ï…Î¬Î´ÎµÏ‚": int(len(broken_df))})
    summary = pd.DataFrame(summary_rows).sort_values("Î£ÎµÎ½Î¬ÏÎ¹Î¿ (sheet)")
    st.dataframe(summary, use_container_width=True)

    st.download_button(
        "â¬‡ï¸ ÎšÎ±Ï„Î­Î²Î±ÏƒÎµ Î±Î½Î±Ï†Î¿ÏÎ¬ (Î Î»Î®ÏÎµÏ‚ Î±Î½Ï„Î¯Î³ÏÎ±Ï†Î¿ + ÏƒÏ€Î±ÏƒÎ¼Î­Î½ÎµÏ‚ + ÏƒÏÎ½Î¿ÏˆÎ·)",
        data=build_broken_report(xl, mode="full").getvalue(),
        file_name=f"broken_friends_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

    # Detailed preview + diagnostics
    with st.expander("ğŸ” Î ÏÎ¿Î²Î¿Î»Î® Î±Î½Î±Î»Ï…Ï„Î¹ÎºÏÎ½ Î¶ÎµÏ…Î³ÏÎ½ & Î´Î¹Î¬Î³Î½Ï‰ÏƒÎ· Î±Î½Î¬ sheet"):
        for sheet in xl.sheet_names:
            df_raw = xl.parse(sheet_name=sheet)
            df_norm, _ = auto_rename_columns(df_raw)  # âœ… fixed: correct function name only
            broken_df, diag = friend_resolution_diagnostics(df_norm)
            st.markdown(f"**{sheet}**")
            if broken_df.empty:
                st.info("â€” ÎšÎ±Î¼Î¯Î± ÏƒÏ€Î±ÏƒÎ¼Î­Î½Î· Ï€Î»Î®ÏÏ‰Ï‚ Î±Î¼Î¿Î¹Î²Î±Î¯Î± Î´Ï…Î¬Î´Î± â€”")
            else:
                st.dataframe(broken_df, use_container_width=True)
            if diag["unmatched"] or diag["ambiguous"]:
                with st.expander("ğŸ› ï¸ Î”Î¹Î¬Î³Î½Ï‰ÏƒÎ· Î±Î½Ï„Î¹ÏƒÏ„Î¿Î¯Ï‡Î¹ÏƒÎ·Ï‚ Î¿Î½Î¿Î¼Î¬Ï„Ï‰Î½"):
                    if diag["unmatched"]:
                        st.warning("ÎœÎ· Î±Î½Ï„Î¹ÏƒÏ„Î¿Î¹Ï‡Î¹ÏƒÎ¼Î­Î½Î± Î¿Î½ÏŒÎ¼Î±Ï„Î± Ï†Î¯Î»Ï‰Î½: " + ", ".join(diag["unmatched"]))
                    if diag["ambiguous"]:
                        st.error("Î‘Î¼Ï†Î¯Î²Î¿Î»Î± Î¿Î½ÏŒÎ¼Î±Ï„Î± (Î¯Î´Î¹Î¿ Î¼Î¹ÎºÏÏŒ/ÎµÏ€ÏÎ½Ï…Î¼Î¿ ÏƒÎµ Ï€Î¿Î»Î»Î¿ÏÏ‚):")
                        for tok, cand in diag["ambiguous"].items():
                            st.write(f"- **{tok}** â†’ Ï€Î¹Î¸Î±Î½Î¿Î¯: {', '.join(cand)}")


with st.tabs(["ğŸ“Š Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ (1 sheet)", "ğŸ§© Î£Ï€Î±ÏƒÎ¼Î­Î½ÎµÏ‚ Î±Î¼Î¿Î¹Î²Î±Î¯ÎµÏ‚ (ÏŒÎ»Î± Ï„Î± sheets) â€” ÎˆÎ¾Î¿Î´Î¿Ï‚: Î Î»Î®ÏÎµÏ‚ Î±Î½Ï„Î¯Î³ÏÎ±Ï†Î¿ + Î£ÏÎ½Î¿ÏˆÎ·", "ğŸš« Î£Ï…Î³ÎºÏÎ¿ÏÏƒÎµÎ¹Ï‚ (ÏŒÎ»Î± Ï„Î± sheets)"])[2]:
    st.subheader("ğŸš« Î‘Î½Î±Ï†Î¿ÏÎ¬ Î£Ï…Î³ÎºÏÎ¿ÏÏƒÎµÏ‰Î½ ÏƒÏ„Î·Î½ Î¯Î´Î¹Î± Ï„Î¬Î¾Î· (ÏŒÎ»Î± Ï„Î± sheets)")
    # Summary per sheet
    sum_rows = []
    pairs_by_sheet = {}
    for sheet in xl.sheet_names:
        df_raw = xl.parse(sheet_name=sheet)
        df_norm, _ = auto_rename_columns(df_raw)
        counts, pairs, _ = compute_conflict_counts_and_pairs(df_norm)
        sum_rows.append({"Î£ÎµÎ½Î¬ÏÎ¹Î¿ (sheet)": sheet, "Î–ÎµÏÎ³Î· ÏƒÏÎ³ÎºÏÎ¿Ï…ÏƒÎ·Ï‚ ÏƒÏ„Î·Î½ Î¯Î´Î¹Î± Ï„Î¬Î¾Î·": int(len(pairs))})
        pairs_by_sheet[sheet] = pairs
    summ_conf = pd.DataFrame(sum_rows).sort_values("Î£ÎµÎ½Î¬ÏÎ¹Î¿ (sheet)")
    st.dataframe(summ_conf, use_container_width=True)
    # Detailed per sheet
    with st.expander("ğŸ” Î‘Î½Î±Î»Ï…Ï„Î¹ÎºÎ¬ Î¶ÎµÏÎ³Î· Î±Î½Î¬ sheet"):
        for sheet in xl.sheet_names:
            st.markdown(f"**{sheet}**")
            pairs = pairs_by_sheet[sheet]
            if pairs.empty:
                st.info("â€” Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î¶ÎµÏÎ³Î· ÏƒÏÎ³ÎºÏÎ¿Ï…ÏƒÎ·Ï‚ ÏƒÏ„Î·Î½ Î¯Î´Î¹Î± Ï„Î¬Î¾Î· â€”")
            else:
                st.dataframe(pairs, use_container_width=True)

