
import streamlit as st
import pandas as pd
from io import BytesIO
from datetime import datetime

st.set_page_config(page_title="ğŸ“Š Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ ÎœÎ±Î¸Î·Ï„ÏÎ½ Î‘' Î”Î·Î¼Î¿Ï„Î¹ÎºÎ¿Ï", page_icon="ğŸ“Š", layout="wide")
st.title("ğŸ“Š Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ ÎœÎ±Î¸Î·Ï„ÏÎ½ Î‘' Î”Î·Î¼Î¿Ï„Î¹ÎºÎ¿Ï")

# ---------------------------
# Sidebar: Legal / Terms
# ---------------------------
st.sidebar.markdown("### âš–ï¸ ÎŒÏÎ¿Î¹ Ï‡ÏÎ®ÏƒÎ·Ï‚")
terms_ok = st.sidebar.checkbox("Î‘Ï€Î¿Î´Î­Ï‡Î¿Î¼Î±Î¹ Ï„Î¿Ï…Ï‚ ÏŒÏÎ¿Ï…Ï‚ Ï‡ÏÎ®ÏƒÎ·Ï‚", value=False)
st.sidebar.markdown("Â© 2025 â€¢ Î Î½ÎµÏ…Î¼Î±Ï„Î¹ÎºÎ¬ Î´Î¹ÎºÎ±Î¹ÏÎ¼Î±Ï„Î± â€¢ **Î Î±Î½Î±Î³Î¹ÏÏ„Î± Î“Î¹Î±Î½Î½Î¯Ï„ÏƒÎ±ÏÎ¿Ï…**")

# Owner / License quick section
with st.sidebar.expander("ÎšÎ¬Ï„Î¿Ï‡Î¿Ï‚/Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÏŒÏ‚ & Î†Î´ÎµÎ¹Î±", expanded=False):
    st.markdown("""
**ÎšÎ¬Ï„Î¿Ï‡Î¿Ï‚/Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÏŒÏ‚:** Î Î±Î½Î±Î³Î¹ÏÏ„Î± Î“Î¹Î±Î½Î½Î¯Ï„ÏƒÎ±ÏÎ¿Ï…  
**Î ÏÎ¿ÏŠÏŒÎ½:** Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬/ÎšÎ±Ï„Î±Î½Î¿Î¼Î® ÎœÎ±Î¸Î·Ï„ÏÎ½ Î‘Î„ Î”Î·Î¼Î¿Ï„Î¹ÎºÎ¿Ï  

- Î— ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Ï€ÏÎ¿Î¿ÏÎ¯Î¶ÎµÏ„Î±Î¹ Î±Ï€Î¿ÎºÎ»ÎµÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î³Î¹Î± **ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÎ® Ï‡ÏÎ®ÏƒÎ·** Î±Ï€ÏŒ ÏƒÏ‡Î¿Î»Î¹ÎºÎ­Ï‚ Î¼Î¿Î½Î¬Î´ÎµÏ‚/ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÎ¿ÏÏ‚.  
- **Î Î½ÎµÏ…Î¼Î±Ï„Î¹ÎºÎ¬ Î´Î¹ÎºÎ±Î¹ÏÎ¼Î±Ï„Î±:** Â© 2025 Î Î±Î½Î±Î³Î¹ÏÏ„Î± Î“Î¹Î±Î½Î½Î¯Ï„ÏƒÎ±ÏÎ¿Ï…. **Î‘Ï€Î±Î³Î¿ÏÎµÏÎµÏ„Î±Î¹** Î±Î½Ï„Î¹Î³ÏÎ±Ï†Î®, Î±Î½Î±Î´Î·Î¼Î¿ÏƒÎ¯ÎµÏ…ÏƒÎ· Î® Ï„ÏÎ¿Ï€Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï‡Ï‰ÏÎ¯Ï‚ **Î­Î³Î³ÏÎ±Ï†Î· Î¬Î´ÎµÎ¹Î±**.  
- **ÎœÎ· ÎµÎ¼Ï€Î¿ÏÎ¹ÎºÎ® Ï‡ÏÎ®ÏƒÎ·** ÎµÏ€Î¹Ï„ÏÎ­Ï€ÎµÏ„Î±Î¹ ÏƒÎµ ÏƒÏ‡Î¿Î»ÎµÎ¯Î± Î³Î¹Î± ÎµÏƒÏ‰Ï„ÎµÏÎ¹ÎºÎ® Î¿ÏÎ³Î¬Î½Ï‰ÏƒÎ·.  
- Î Î±ÏÎ­Ï‡ÎµÏ„Î±Î¹ â€œ**Ï‰Ï‚ Î­Ï‡ÎµÎ¹**â€ Ï‡Ï‰ÏÎ¯Ï‚ ÎµÎ³Î³Ï…Î®ÏƒÎµÎ¹Ï‚. Î¤Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î­Ï‡Î¿Ï…Î½ **Î²Î¿Î·Î¸Î·Ï„Î¹ÎºÏŒ** Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÎ± ÎºÎ±Î¹ **Î´ÎµÎ½ Ï…Ï€Î¿ÎºÎ±Î¸Î¹ÏƒÏ„Î¿ÏÎ½** ÎºÎ±Î½Î¿Î½Î¹ÏƒÏ„Î¹ÎºÎ­Ï‚ Î±Ï€Î¿Ï†Î¬ÏƒÎµÎ¹Ï‚ Î® Ï€Î±Î¹Î´Î±Î³Ï‰Î³Î¹ÎºÎ® ÎºÏÎ¯ÏƒÎ·.  
- **Î•Ï€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î±:** [panayiotayiannitsarou@gmail.com](mailto:panayiotayiannitsarou@gmail.com)
""")

# Data Protection (GDPR) Guidance
with st.sidebar.expander("ğŸ”’ Î ÏÎ¿ÏƒÏ„Î±ÏƒÎ¯Î± Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ (GDPR â€“ ÎšÏÏ€ÏÎ¿Ï‚)", expanded=False):
    st.markdown("""
- Î¤Î± Î±ÏÏ‡ÎµÎ¯Î± Excel Î±Î½ÎµÎ²Î±Î¯Î½Î¿Ï…Î½ Î±Ï€ÏŒ Ï„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î· ÎºÎ±Î¹ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ½Ï„Î±Î¹ **Î¼ÏŒÎ½Î¿** Î³Î¹Î± Î¬Î¼ÎµÏƒÎ¿ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒ. Î— ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Î´ÎµÎ½ Î±Ï€Î¿Î¸Î·ÎºÎµÏÎµÎ¹ Î¼ÏŒÎ½Î¹Î¼Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±.  
- ÎŸ Ï‡ÏÎ®ÏƒÏ„Î·Ï‚/ÏƒÏ‡Î¿Î»ÎµÎ¯Î¿ ÎµÏ…Î¸ÏÎ½ÎµÏ„Î±Î¹ Î³Î¹Î± ÏƒÏ…Î¼Î¼ÏŒÏÏ†Ï‰ÏƒÎ· Î¼Îµ **GDPR** (Î†ÏÎ¸ÏÎ¿ 5: Î‘ÏÏ‡Î­Ï‚, Î†ÏÎ¸ÏÎ¿ 6: ÎÎ¿Î¼Î¹ÎºÎ® Î²Î¬ÏƒÎ·).  
- **Î£Ï…ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚:**  
  â€¢ Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ **ÏˆÎµÏ…Î´ÏÎ½Ï…Î¼Î±/ÎºÏ‰Î´Î¹ÎºÎ¿ÏÏ‚** (Ï€.Ï‡. Î‘1_001) Î±Î½Ï„Î¯ Ï€Î»Î®ÏÏ‰Î½ Î¿Î½Î¿Î¼Î¬Ï„Ï‰Î½, ÎµÏ†ÏŒÏƒÎ¿Î½ Î³Î¯Î½ÎµÏ„Î±Î¹.  
  â€¢ Î•Ï†Î±ÏÎ¼ÏŒÏƒÏ„Îµ **ÎµÎ»Î±Ï‡Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ·** (Î¼ÏŒÎ½Î¿ Ï„Î± Î±Ï€Î¿Î»ÏÏ„Ï‰Ï‚ Î±Î½Î±Î³ÎºÎ±Î¯Î± Ï€ÎµÎ´Î¯Î±).  
  â€¢ ÎšÎ±Î¸Î¿ÏÎ¯ÏƒÏ„Îµ **Î´Î¹Î¬ÏƒÏ„Î·Î¼Î± Î´Î¹Î±Ï„Î®ÏÎ·ÏƒÎ·Ï‚** ÎºÎ±Î¹ Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± **Î´Î¹Î±Î³ÏÎ±Ï†Î®Ï‚**.  
  â€¢ Î•Î½Î·Î¼ÎµÏÏÏƒÏ„Îµ Ï„Î¿Î½/Ï„Î·Î½ **Î¥Ï€ÎµÏÎ¸Ï…Î½Î¿ Î ÏÎ¿ÏƒÏ„Î±ÏƒÎ¯Î±Ï‚ Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ (DPO)** Ï„Î¿Ï… ÏƒÏ‡Î¿Î»ÎµÎ¯Î¿Ï….  
  â€¢ Î‘Î½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎµÏ„Îµ **cloud** (Ï€.Ï‡. Streamlit Cloud): ÎµÎ»Î­Î³Î¾Ï„Îµ Ï€Î¿Ï Ï†Î¹Î»Î¿Î¾ÎµÎ½Î¿ÏÎ½Ï„Î±Î¹ Î¿Î¹ Ï…Ï€Î¿Î´Î¿Î¼Î­Ï‚, Ï„Î¿Ï…Ï‚ ÏŒÏÎ¿Ï…Ï‚ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚ ÎºÎ±Î¹ Î±Ï€Î¿Ï†ÏÎ³ÎµÏ„Îµ Ï„Î·Î½ Î±Î½Î¬ÏÏ„Î·ÏƒÎ· **Î±Î½Î±Î³Î½Ï‰ÏÎ¯ÏƒÎ¹Î¼Ï‰Î½** ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Ï‰Î½ ÏŒÏ€Î¿Ï… Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„Î¿.
""")

if not terms_ok:
    st.warning("âš ï¸ Î“Î¹Î± Î½Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹Ï‚ Ï„Î·Î½ ÎµÏ†Î±ÏÎ¼Î¿Î³Î®, Î±Ï€Î¿Î´Î­Î¾Î¿Ï… Ï„Î¿Ï…Ï‚ ÏŒÏÎ¿Ï…Ï‚ Ï‡ÏÎ®ÏƒÎ·Ï‚ (Î±ÏÎ¹ÏƒÏ„ÎµÏÎ¬).")
    st.stop()

# ---------------------------
# Full Terms (main page expander)
# ---------------------------
with st.expander("ğŸ“œ Î Î»Î®ÏÎµÎ¹Ï‚ ÎŒÏÎ¿Î¹ Î§ÏÎ®ÏƒÎ·Ï‚ & Î‘Ï€Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î•Ï…Î¸ÏÎ½Î·Ï‚", expanded=False):
    st.markdown("""
1) **Î£ÎºÎ¿Ï€ÏŒÏ‚:** Î¥Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î· ÎµÏƒÏ‰Ï„ÎµÏÎ¹ÎºÎ¿Ï Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÎ¼Î¿Ï/ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÏÎ½ Ï„Î¬Î¾ÎµÏ‰Î½ Î‘Î„ Î”Î·Î¼Î¿Ï„Î¹ÎºÎ¿Ï.  
2) **Î™Î´Î¹Ï‰Ï„Î¹ÎºÏŒÏ„Î·Ï„Î± & Î”ÎµÎ´Î¿Î¼Î­Î½Î±:** Î¤Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï€Î±ÏÎ­Ï‡Î¿Î½Ï„Î±Î¹ Î±Ï€Î¿ÎºÎ»ÎµÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î±Ï€ÏŒ Ï„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î·. Î”ÎµÎ½ Î±Ï€Î¿Î¸Î·ÎºÎµÏÎ¿Î½Ï„Î±Î¹ Î¼ÏŒÎ½Î¹Î¼Î± Î±Ï€ÏŒ Ï„Î·Î½ ÎµÏ†Î±ÏÎ¼Î¿Î³Î®, Î¿ÏÏ„Îµ Î´Î¹Î±Î¼Î¿Î¹ÏÎ¬Î¶Î¿Î½Ï„Î±Î¹ ÏƒÎµ Ï„ÏÎ¯Ï„Î¿Ï…Ï‚. ÎŸ Ï‡ÏÎ®ÏƒÏ„Î·Ï‚ Ï€Î±ÏÎ±Î¼Î­Î½ÎµÎ¹ Ï…Ï€ÎµÏÎ¸Ï…Î½Î¿Ï‚ Î³Î¹Î± Ï„Î· ÏƒÏ…Î¼Î¼ÏŒÏÏ†Ï‰ÏƒÎ· Î¼Îµ Ï„Î¿Î½ **GDPR** ÎºÎ±Î¹ Ï„Î·Î½ ÎµÎ¸Î½Î¹ÎºÎ® Î½Î¿Î¼Î¿Î¸ÎµÏƒÎ¯Î± (Ï€.Ï‡. ÏˆÎµÏ…Î´Ï‰Î½Ï…Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎ·).  
3) **Î ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼Î¿Î¯:** Î‘Ï€Î±Î³Î¿ÏÎµÏÎµÏ„Î±Î¹ Î· ÎµÎ¼Ï€Î¿ÏÎ¹ÎºÎ® ÎµÎºÎ¼ÎµÏ„Î¬Î»Î»ÎµÏ…ÏƒÎ·, Î´Î¹Î¬Î¸ÎµÏƒÎ· ÏƒÎµ Ï„ÏÎ¯Ï„Î¿Ï…Ï‚ ÎºÎ±Î¹ Ï„ÏÎ¿Ï€Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï‡Ï‰ÏÎ¯Ï‚ Î¬Î´ÎµÎ¹Î±.  
4) **Î‘Ï€Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î•Ï…Î¸ÏÎ½Î·Ï‚:** ÎŸ/Î— Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÏŒÏ‚ Î´ÎµÎ½ ÎµÏ…Î¸ÏÎ½ÎµÏ„Î±Î¹ Î³Î¹Î± Î±Ï€Î¿Ï†Î¬ÏƒÎµÎ¹Ï‚ Ï€Î¿Ï… Î»Î±Î¼Î²Î¬Î½Î¿Î½Ï„Î±Î¹ Î±Ï€Î¿ÎºÎ»ÎµÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î® Î³Î¹Î± ÏƒÏ†Î¬Î»Î¼Î±Ï„Î±/ÎµÎ»Î»ÎµÎ¯ÏˆÎµÎ¹Ï‚ ÏƒÏ„Î± ÎµÎ¹ÏƒÎ±Î³ÏŒÎ¼ÎµÎ½Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±.  
5) **Î¤ÏÎ¿Ï€Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹Ï‚:** Î— ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÎ½Î·Î¼ÎµÏÏÎ½ÎµÏ„Î±Î¹ Ï‡Ï‰ÏÎ¯Ï‚ Ï€ÏÎ¿ÎµÎ¹Î´Î¿Ï€Î¿Î¯Î·ÏƒÎ·.  
6) **Î‘Ï€Î¿Î´Î¿Ï‡Î®:** Î— Ï‡ÏÎ®ÏƒÎ· ÏƒÏ…Î½ÎµÏ€Î¬Î³ÎµÏ„Î±Î¹ Ï€Î»Î®ÏÎ· Î±Ï€Î¿Î´Î¿Ï‡Î® Ï„Ï‰Î½ ÏŒÏÏ‰Î½.
""")

# ---------------------------
# Admin Checklist (directors)
# ---------------------------
with st.expander("âœ… Checklist Î³Î¹Î± Î”Î¹ÎµÏ…Î¸Ï…Î½Ï„Î­Ï‚/ÏÎ¹ÎµÏ‚ Î£Ï‡Î¿Î»ÎµÎ¯Ï‰Î½", expanded=False):
    st.markdown("""
- [ ] ÎˆÏ‡ÎµÎ¹ ÎµÎ½Î·Î¼ÎµÏÏ‰Î¸ÎµÎ¯ Î¿/Î· **DPO**;  
- [ ] Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ½Ï„Î±Î¹ **ÎºÏ‰Î´Î¹ÎºÎ¿Î¯/ÏˆÎµÏ…Î´ÏÎ½Ï…Î¼Î±** Î±Î½Ï„Î¯ Î³Î¹Î± Ï€Î»Î®ÏÎ· Î¿Î½ÏŒÎ¼Î±Ï„Î± ÏŒÏ€Î¿Ï… ÎµÎ¯Î½Î±Î¹ ÎµÏ†Î¹ÎºÏ„ÏŒ;  
- [ ] Î¥Ï€Î¬ÏÏ‡ÎµÎ¹ **Î½Î¿Î¼Î¹ÎºÎ® Î²Î¬ÏƒÎ·** ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚ (Î†ÏÎ¸ÏÎ¿ 6 GDPR â€“ Ï€.Ï‡. Î±Ï€Î¿ÏƒÏ„Î¿Î»Î® Î´Î·Î¼ÏŒÏƒÎ¹Î¿Ï… ÎºÎ±Î¸Î®ÎºÎ¿Î½Ï„Î¿Ï‚/Î½ÏŒÎ¼Î¹Î¼Î¿ ÏƒÏ…Î¼Ï†Î­ÏÎ¿Î½);  
- [ ] Î•Ï†Î±ÏÎ¼ÏŒÎ¶ÎµÏ„Î±Î¹ **ÎµÎ»Î±Ï‡Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ·** ÎºÎ±Î¹ **Î±Î½Î¬Î³ÎºÎ· Î³Î½ÏÏƒÎ·Ï‚** (Î¼ÏŒÎ½Î¿ Ï„Î± Î±Ï€Î¿Î»ÏÏ„Ï‰Ï‚ Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±/Ï€ÏÏŒÏƒÎ²Î±ÏƒÎ·);  
- [ ] ÎŸÏÎ¯Î¶ÎµÏ„Î±Î¹ **Ï€ÎµÏÎ¯Î¿Î´Î¿Ï‚ Î´Î¹Î±Ï„Î®ÏÎ·ÏƒÎ·Ï‚** ÎºÎ±Î¹ Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± **Î´Î¹Î±Î³ÏÎ±Ï†Î®Ï‚** Î±ÏÏ‡ÎµÎ¯Ï‰Î½;  
- [ ] Î‘Î½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ **cloud**, Î­Ï‡ÎµÎ¹ ÎµÎ»ÎµÎ³Ï‡Î¸ÎµÎ¯ Î¿ Ï€Î¬ÏÎ¿Ï‡Î¿Ï‚/Ï„Î¿Ï€Î¿Î¸ÎµÏƒÎ¯Î±/ÏŒÏÎ¿Î¹ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚;  
- [ ] Î¥Ï€Î¬ÏÏ‡ÎµÎ¹ **ÎµÎ½Î·Î¼Î­ÏÏ‰ÏƒÎ·** Ï€ÏÎ¿Ï‚ Î³Î¿Î½ÎµÎ¯Ï‚/ÎºÎ·Î´ÎµÎ¼ÏŒÎ½ÎµÏ‚, ÎµÏ†ÏŒÏƒÎ¿Î½ Î±Ï€Î±Î¹Ï„ÎµÎ¯Ï„Î±Î¹, ÏƒÏ‡ÎµÏ„Î¹ÎºÎ¬ Î¼Îµ Ï„Î¿Î½ Ï„ÏÏŒÏ€Î¿ Ï‡ÏÎ®ÏƒÎ·Ï‚ Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½;
""")

# ---------------------------
# Session State
# ---------------------------
for key, default in [("data", None), ("stats_df", None), ("show_upload", False), ("diagnostics", {})]:
    if key not in st.session_state:
        st.session_state[key] = default

# ---------- Column normalization ----------
def canon(s: str) -> str:
    return "".join((s or "").replace("_"," ").split()).upper()

CANON_TARGETS = {
    "ÎŸÎÎŸÎœÎ‘": {"ÎŸÎÎŸÎœÎ‘"},
    "Î¦Î¥Î›ÎŸ": {"Î¦Î¥Î›ÎŸ"},
    "Î Î‘Î™Î”Î™_Î•ÎšÎ Î‘Î™Î”Î•Î¥Î¤Î™ÎšÎŸÎ¥": {"Î Î‘Î™Î”Î™Î•ÎšÎ Î‘Î™Î”Î•Î¥Î¤Î™ÎšÎŸÎ¥", "Î Î‘Î™Î”Î™-Î•ÎšÎ Î‘Î™Î”Î•Î¥Î¤Î™ÎšÎŸÎ¥"},
    "Î–Î©Î—Î¡ÎŸÎ£": {"Î–Î©Î—Î¡ÎŸÎ£"},
    "Î™Î”Î™Î‘Î™Î¤Î•Î¡ÎŸÎ¤Î—Î¤Î‘": {"Î™Î”Î™Î‘Î™Î¤Î•Î¡ÎŸÎ¤Î—Î¤Î‘"},
    "ÎšÎ‘Î›Î—_Î“ÎÎ©Î£Î—_Î•Î›Î›Î—ÎÎ™ÎšÎ©Î": {"ÎšÎ‘Î›Î—Î“ÎÎ©Î£Î—Î•Î›Î›Î—ÎÎ™ÎšÎ©Î", "Î“ÎÎ©Î£Î—Î•Î›Î›Î—ÎÎ™ÎšÎ©Î"},
    "Î¦Î™Î›ÎŸÎ™": {"Î¦Î™Î›ÎŸÎ™", "Î¦Î™Î›Î™Î‘"},
    "Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—": {"Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—", "Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î•Î™Î£"},
    "Î¤ÎœÎ—ÎœÎ‘": {"Î¤ÎœÎ—ÎœÎ‘"},
}

REQUIRED_COLS = ["ÎŸÎÎŸÎœÎ‘","Î¦Î¥Î›ÎŸ","Î Î‘Î™Î”Î™_Î•ÎšÎ Î‘Î™Î”Î•Î¥Î¤Î™ÎšÎŸÎ¥","Î–Î©Î—Î¡ÎŸÎ£","Î™Î”Î™Î‘Î™Î¤Î•Î¡ÎŸÎ¤Î—Î¤Î‘","ÎšÎ‘Î›Î—_Î“ÎÎ©Î£Î—_Î•Î›Î›Î—ÎÎ™ÎšÎ©Î","Î¦Î™Î›ÎŸÎ™","Î£Î¥Î“ÎšÎ¡ÎŸÎ¥Î£Î—","Î¤ÎœÎ—ÎœÎ‘"]


def auto_rename_columns(df: pd.DataFrame):
    """
    Rename common Greek columns to a canonical set.
    Now more permissive for the 'Î¦Î™Î›ÎŸÎ™' column: detects any column whose canonical contains 'Î¦Î™Î›'.
    Also accepts 'FRIEND'/'FRIENDS'.
    """
    mapping = {}
    seen = set()
    for col in df.columns:
        c = _canon(col)
        found_target = None
        for target, keys in CANON_TARGETS.items():
            if c in keys and target not in seen:
                found_target = target
                seen.add(target)
                break
        if found_target:
            mapping[col] = found_target

    renamed = df.rename(columns=mapping)

    # Fallbacks for friends column if not mapped already
    friends_cols = [c for c in renamed.columns if c in ("Î¦Î™Î›ÎŸÎ™","Î¦Î™Î›Î™Î‘","Î¦Î™Î›ÎŸÎ£")]
    if not friends_cols:
        # search candidates by substring
        candidates = []
        for col in df.columns:
            c = _canon(col)
            if "Î¦Î™Î›" in c or "FRIEND" in c:
                candidates.append(col)
        # If we find multiple friend-like columns, concatenate them into one
        if candidates:
            # Create a combined Î¦Î™Î›ÎŸÎ™ column by joining non-empty parts with commas
            combined = []
            for _, row in df[candidates].astype(str).iterrows():
                vals = [str(v).strip() for v in row.tolist() if str(v).strip() and str(v).strip().upper() not in ("-", "NA", "NAN")]
                combined.append(", ".join(vals))
            renamed["Î¦Î™Î›ÎŸÎ™"] = combined

    # Ensure Î¤ÎœÎ—ÎœÎ‘ exists: if not, try to pick the rightmost column that looks like class labels (A1, A2...)
    if "Î¤ÎœÎ—ÎœÎ‘" not in renamed.columns:
        # candidate: column with short string labels and <= 6 unique non-empty values
        best = None
        for col in df.columns[::-1]:
            s = df[col].dropna().astype(str).str.strip()
            if not len(s):
                continue
            if s.str.len().median() <= 4 and s.nunique() <= 10:
                best = col
                break
        if best:
            renamed = renamed.rename(columns={best: "Î¤ÎœÎ—ÎœÎ‘"})

    return renamed, mapping


def _normalize_yes_no(series: pd.Series) -> pd.Series:
    if series.dtype == object:
        s = series.fillna("").astype(str).str.strip().str.upper()
        s = s.replace({
            "ÎÎ‘Î™":"Î","NAI":"Î","YES":"Î","Y":"Î",
            "ÎŸÎ§Î™":"ÎŸ","OXI":"ÎŸ","NO":"ÎŸ","N":"ÎŸ","": "ÎŸ"
        })
        return s.where(s.isin(["Î","ÎŸ"]), other="ÎŸ")
    return series

def _broken_mutual_friendships_per_class(df: pd.DataFrame) -> pd.Series:
    fcol = "Î¦Î™Î›ÎŸÎ™" if "Î¦Î™Î›ÎŸÎ™" in df.columns else None
    if fcol is None or "ÎŸÎÎŸÎœÎ‘" not in df.columns or "Î¤ÎœÎ—ÎœÎ‘" not in df.columns:
        return df.groupby("Î¤ÎœÎ—ÎœÎ‘").size() * 0

    def norm_name(x: str) -> str:
        return (x or "").strip()

    names = df["ÎŸÎÎŸÎœÎ‘"].fillna("").astype(str).apply(norm_name)
    class_by_name = dict(zip(names, df["Î¤ÎœÎ—ÎœÎ‘"]))

    friends_map = {}
    for _, row in df.iterrows():
        me = norm_name(str(row.get("ÎŸÎÎŸÎœÎ‘", "")))
        raw = str(row.get(fcol, "") or "")
        flist = [norm_name(p) for p in raw.split(",") if norm_name(p)]
        friends_map[me] = set(flist)

    mutual_pairs = set()
    for a, flist in friends_map.items():
        for b in flist:
            if b in friends_map and a in friends_map[b]:
                mutual_pairs.add(tuple(sorted([a,b])))

    broken_count_by_class = {tmima: 0 for tmima in df["Î¤ÎœÎ—ÎœÎ‘"].dropna().unique()}
    for a, b in mutual_pairs:
        ta = class_by_name.get(a)
        tb = class_by_name.get(b)
        if ta and tb and ta != tb:
            broken_count_by_class[ta] = broken_count_by_class.get(ta, 0) + 1
            broken_count_by_class[tb] = broken_count_by_class.get(tb, 0) + 1

    return pd.Series(broken_count_by_class)

def _generate_stats(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # Trim only strings, preserve NaN so groupby ignores empty classes
    if "Î¤ÎœÎ—ÎœÎ‘" in df:
        df["Î¤ÎœÎ—ÎœÎ‘"] = df["Î¤ÎœÎ—ÎœÎ‘"].apply(lambda v: v.strip() if isinstance(v, str) else v)
    if "Î¦Î¥Î›ÎŸ" in df:
        df["Î¦Î¥Î›ÎŸ"] = df["Î¦Î¥Î›ÎŸ"].fillna("").astype(str).str.strip().str.upper()
    for col in ["Î Î‘Î™Î”Î™_Î•ÎšÎ Î‘Î™Î”Î•Î¥Î¤Î™ÎšÎŸÎ¥","Î–Î©Î—Î¡ÎŸÎ£","Î™Î”Î™Î‘Î™Î¤Î•Î¡ÎŸÎ¤Î—Î¤Î‘","ÎšÎ‘Î›Î—_Î“ÎÎ©Î£Î—_Î•Î›Î›Î—ÎÎ™ÎšÎ©Î"]:
        if col in df:
            df[col] = _normalize_yes_no(df[col])

    boys = df[df["Î¦Î¥Î›ÎŸ"] == "Î‘"].groupby("Î¤ÎœÎ—ÎœÎ‘").size()
    girls = df[df["Î¦Î¥Î›ÎŸ"] == "Îš"].groupby("Î¤ÎœÎ—ÎœÎ‘").size()
    educators = df[df["Î Î‘Î™Î”Î™_Î•ÎšÎ Î‘Î™Î”Î•Î¥Î¤Î™ÎšÎŸÎ¥"] == "Î"].groupby("Î¤ÎœÎ—ÎœÎ‘").size()
    energetic = df[df["Î–Î©Î—Î¡ÎŸÎ£"] == "Î"].groupby("Î¤ÎœÎ—ÎœÎ‘").size()
    special = df[df["Î™Î”Î™Î‘Î™Î¤Î•Î¡ÎŸÎ¤Î—Î¤Î‘"] == "Î"].groupby("Î¤ÎœÎ—ÎœÎ‘").size()
    greek = df[df["ÎšÎ‘Î›Î—_Î“ÎÎ©Î£Î—_Î•Î›Î›Î—ÎÎ™ÎšÎ©Î"] == "Î"].groupby("Î¤ÎœÎ—ÎœÎ‘").size()
    total = df.groupby("Î¤ÎœÎ—ÎœÎ‘").size()

    broken_by_class = _broken_mutual_friendships_per_class(df)

    stats = pd.DataFrame({
        "Î‘Î“ÎŸÎ¡Î™Î‘": boys,
        "ÎšÎŸÎ¡Î™Î¤Î£Î™Î‘": girls,
        "Î Î‘Î™Î”Î™_Î•ÎšÎ Î‘Î™Î”Î•Î¥Î¤Î™ÎšÎŸÎ¥": educators,
        "Î–Î©Î—Î¡ÎŸÎ™": energetic,
        "Î™Î”Î™Î‘Î™Î¤Î•Î¡ÎŸÎ¤Î—Î¤Î‘": special,
        "Î“ÎÎ©Î£Î— Î•Î›Î›Î—ÎÎ™ÎšÎ©Î": greek,
        "Î£Î Î‘Î£ÎœÎ•ÎÎ— Î¦Î™Î›Î™Î‘": broken_by_class,
        "Î£Î¥ÎÎŸÎ›ÎŸ ÎœÎ‘Î˜Î—Î¤Î©Î": total,
    }).fillna(0).astype(int)

    # Safety: if for ÎºÎ¬Ï€Î¿Î¹Î¿ Î»ÏŒÎ³Î¿ Ï€ÏÎ¿Î­ÎºÏ…ÏˆÎµ string 'nan' Î±Ï€ÏŒ Ï€Î±Î»Î±Î¹ÏŒ Î±ÏÏ‡ÎµÎ¯Î¿, ÎºÏÏÏˆ' Ï„Î¿
    if hasattr(stats.index, "str"):
        stats = stats.loc[stats.index.str.lower() != "nan"]

    try:
        stats = stats.sort_index(key=lambda x: x.str.extract(r"(\d+)")[0].astype(float))
    except Exception:
        stats = stats.sort_index()

    return stats

def _export_to_excel(stats_df: pd.DataFrame) -> BytesIO:
    output = BytesIO()
    try:
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            stats_df.to_excel(writer, index=True, sheet_name="Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬", index_label="Î¤ÎœÎ—ÎœÎ‘")
            wb = writer.book
            ws = writer.sheets["Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬"]
            header_fmt = wb.add_format({"bold": True, "valign":"vcenter", "text_wrap": True, "border":1})
            for col_idx, value in enumerate(["Î¤ÎœÎ—ÎœÎ‘"] + list(stats_df.columns)):
                ws.write(0, col_idx, value, header_fmt)
            for i in range(0, len(stats_df.columns)+1):
                ws.set_column(i, i, 18)
    except Exception:
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            stats_df.to_excel(writer, index=True, sheet_name="Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬", index_label="Î¤ÎœÎ—ÎœÎ‘")
    output.seek(0)
    return output

# ---------------------------
# UI Buttons
# ---------------------------
c1, c2, c3 = st.columns(3)
with c1:
    if st.button("ğŸ“¥ Î•Î¹ÏƒÎ±Î³Ï‰Î³Î® Excel", type="primary"):
        st.session_state.show_upload = True
with c2:
    export_clicked = st.button("ğŸ“Š Î•Î¾Î±Î³Ï‰Î³Î® Î Î™ÎÎ‘ÎšÎ‘ Î£Î¤Î‘Î¤Î™Î£Î¤Î™ÎšÎ©Î", disabled=st.session_state.data is None)
with c3:
    if st.button("ğŸ”„ Î•Ï€Î±Î½ÎµÎºÎºÎ¯Î½Î·ÏƒÎ·"):
        st.session_state.data = None
        st.session_state.stats_df = None
        st.session_state.show_upload = False
        st.session_state.diagnostics = {}
        st.rerun()

# ---------------------------
# Upload + Diagnostics + Preview
# ---------------------------
if st.session_state.show_upload:
    st.markdown("### ğŸ“¥ Î•Î¹ÏƒÎ±Î³Ï‰Î³Î® Î‘ÏÏ‡ÎµÎ¯Î¿Ï… Excel")
    up = st.file_uploader("Î•Ï€Î¯Î»ÎµÎ¾Îµ Î±ÏÏ‡ÎµÎ¯Î¿ Excel Î¼Îµ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¼Î±Î¸Î·Ï„ÏÎ½", type=["xlsx","xls"])
    if up:
        try:
            df_raw = pd.read_excel(up)
            df_norm, ren_map = auto_rename_columns(df_raw)
            # Trim 'Î¤ÎœÎ—ÎœÎ‘' only if it's string; keep NaN
            if "Î¤ÎœÎ—ÎœÎ‘" in df_norm.columns:
                df_norm["Î¤ÎœÎ—ÎœÎ‘"] = df_norm["Î¤ÎœÎ—ÎœÎ‘"].apply(lambda v: v.strip() if isinstance(v, str) else v)

            st.session_state.data = df_norm.copy()

            present = list(df_norm.columns)
            missing = [c for c in REQUIRED_COLS if c not in present]
            classes = sorted([str(x) for x in df_norm["Î¤ÎœÎ—ÎœÎ‘"].dropna().unique()]) if "Î¤ÎœÎ—ÎœÎ‘" in df_norm else []
            missing_classes = int(df_norm["Î¤ÎœÎ—ÎœÎ‘"].isna().sum()) if "Î¤ÎœÎ—ÎœÎ‘" in df_norm else 0

            st.session_state.diagnostics = {
                "recognized_columns": present,
                "renamed": ren_map,
                "missing_required": missing,
                "classes_found": classes,
                "missing_class_rows": missing_classes,
            }

            st.success(f"âœ… Î•Ï€Î¹Ï„Ï…Ï‡Î®Ï‚ Ï†ÏŒÏÏ„Ï‰ÏƒÎ·! Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(df_norm)} Î¼Î±Î¸Î·Ï„Î­Ï‚.")
            with st.expander("ğŸ” Î”Î¹Î¬Î³Î½Ï‰ÏƒÎ· Î±ÏÏ‡ÎµÎ¯Î¿Ï… (ÏƒÏ„Î®Î»ÎµÏ‚ Ï€Î¿Ï… Î±Î½Î±Î³Î½Ï‰ÏÎ¯ÏƒÏ„Î·ÎºÎ±Î½)", expanded=False):
                st.write("Î‘Î½Î±Î³Î½Ï‰ÏÎ¹ÏƒÎ¼Î­Î½ÎµÏ‚ ÏƒÏ„Î®Î»ÎµÏ‚:", present)
                if ren_map:
                    st.write("Î‘Ï…Ï„ÏŒÎ¼Î±Ï„ÎµÏ‚ Î¼ÎµÏ„Î¿Î½Î¿Î¼Î±ÏƒÎ¯ÎµÏ‚:", ren_map)
                if missing:
                    st.error("âŒ Î›ÎµÎ¯Ï€Î¿Ï…Î½ Ï…Ï€Î¿Ï‡ÏÎµÏ‰Ï„Î¹ÎºÎ­Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚: " + ", ".join(missing))
                if "Î¤ÎœÎ—ÎœÎ‘" in df_norm:
                    st.write("Î¤Î¼Î®Î¼Î±Ï„Î± Ï€Î¿Ï… Î²ÏÎ­Î¸Î·ÎºÎ±Î½:", classes)
                    if missing_classes:
                        st.warning(f"Î¥Ï€Î¬ÏÏ‡Î¿Ï…Î½ {missing_classes} ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ Ï‡Ï‰ÏÎ¯Ï‚ Ï„Î¹Î¼Î® ÏƒÏ„Î¿ Ï€ÎµÎ´Î¯Î¿ Î¤ÎœÎ—ÎœÎ‘ â€” Î´ÎµÎ½ Î¸Î± ÏƒÏ…Î¼Ï€ÎµÏÎ¹Î»Î·Ï†Î¸Î¿ÏÎ½ ÏƒÏ„Î± ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬.")

            if not missing:
                st.markdown("### ğŸ‘€ Î ÏÎ¿ÎµÏ€Î¹ÏƒÎºÏŒÏ€Î·ÏƒÎ· Î Î¯Î½Î±ÎºÎ± Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÏÎ½")
                preview = _generate_stats(df_norm)
                st.session_state.stats_df = preview
                st.dataframe(preview, use_container_width=True)
            else:
                st.info("Î£Ï…Î¼Ï€Î»Î®ÏÏ‰ÏƒÎµ/Î´Î¹ÏŒÏÎ¸Ï‰ÏƒÎµ Ï„Î¹Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚ Ï€Î¿Ï… Î»ÎµÎ¯Ï€Î¿Ï…Î½ ÎºÎ±Î¹ Î¾Î±Î½Î±Ï†ÏŒÏÏ„Ï‰ÏƒÎµ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿.")

        except Exception as e:
            st.error(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î· Ï†ÏŒÏÏ„Ï‰ÏƒÎ·: {e}")

# ---------------------------
# Export
# ---------------------------
if export_clicked and st.session_state.data is not None:
    if st.session_state.diagnostics and st.session_state.diagnostics.get("missing_required"):
        st.error("Î”ÎµÎ½ Î³Î¯Î½ÎµÏ„Î±Î¹ ÎµÎ¾Î±Î³Ï‰Î³Î®: Î»ÎµÎ¯Ï€Î¿Ï…Î½ Ï…Ï€Î¿Ï‡ÏÎµÏ‰Ï„Î¹ÎºÎ­Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚: " + ", ".join(st.session_state.diagnostics["missing_required"]))
    else:
        st.markdown("### ğŸ“Š Î Î¯Î½Î±ÎºÎ±Ï‚ Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÏÎ½")
        stats_df = _generate_stats(st.session_state.data)
        st.session_state.stats_df = stats_df
        st.dataframe(stats_df, use_container_width=True)
        output = _export_to_excel(stats_df)
        st.download_button(
            label="ğŸ’¾ Î›Î®ÏˆÎ· Î Î¯Î½Î±ÎºÎ± Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÏÎ½ (Excel)",
            data=output.getvalue(),
            file_name=f"statistika_mathiton_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            type="primary"
        )

# ---------------------------
# Footer
# ---------------------------
st.markdown("""
â€”  Â© 2025 **Î Î±Î½Î±Î³Î¹ÏÏ„Î± Î“Î¹Î±Î½Î½Î¯Ï„ÏƒÎ±ÏÎ¿Ï…** â€” *Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬/ÎšÎ±Ï„Î±Î½Î¿Î¼Î® ÎœÎ±Î¸Î·Ï„ÏÎ½ Î‘Î„ Î”Î·Î¼Î¿Ï„Î¹ÎºÎ¿Ï*. 
ÎœÏŒÎ½Î¿ Î³Î¹Î± ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÎ® Ï‡ÏÎ®ÏƒÎ·. ÎŒÎ»Î± Ï„Î± Î´Î¹ÎºÎ±Î¹ÏÎ¼Î±Ï„Î± Î´Î¹Î±Ï„Î·ÏÎ¿ÏÎ½Ï„Î±Î¹.
""")




# ===== Appended robust mutual-broken detection (patch FIXED, clean) =====

# === PATCH: Robust mutual-broken detection (CLOUD-SAFE) ===
import re, ast, unicodedata
import pandas as pd

def _strip_diacritics(s: str) -> str:
    nfkd = unicodedata.normalize("NFD", s)
    return "".join(ch for ch in nfkd if not unicodedata.combining(ch))

def _canon_name(s: str) -> str:
    s = (str(s) if s is not None else "").strip()
    s = s.strip("[]'\" ")
    s = re.sub(r"\s+", " ", s)
    s = _strip_diacritics(s).upper()
    return s

def _parse_friends(cell):
    raw = str(cell) if cell is not None else ""
    raw = raw.strip()
    if not raw:
        return []
    if raw.startswith("[") and raw.endswith("]"):
        try:
            val = ast.literal_eval(raw)
            if isinstance(val, (list, tuple)):
                return [_canon_name(x) for x in val if str(x).strip()]
        except Exception:
            pass
        raw2 = raw.strip("[]")
        parts = re.split(r"[;,]", raw2)
        return [_canon_name(p) for p in parts if _canon_name(p)]
    parts = re.split(r"[;,]", raw)
    return [_canon_name(p) for p in parts if _canon_name(p)]

def _broken_mutual_friendships_per_class(df: pd.DataFrame) -> pd.Series:
    """Return Series with count of broken mutual pairs per class.
    Mutual: A lists B AND B lists A (after normalization).
    Broken = mutual pair placed in different non-empty classes.
    Each broken pair is counted once for each involved class.
    """
    alt_fcol = "Î¦Î™Î›ÎŸÎ™" if "Î¦Î™Î›ÎŸÎ™" in df.columns else ("Î¦Î™Î›Î™ÎŸÎ™" if "Î¦Î™Î›Î™ÎŸÎ™" in df.columns else None)
    if alt_fcol is None or "ÎŸÎÎŸÎœÎ‘" not in df.columns or "Î¤ÎœÎ—ÎœÎ‘" not in df.columns:
        return df.groupby("Î¤ÎœÎ—ÎœÎ‘").size() * 0

    df = df.copy()
    df["__CAN_NAME__"] = df["ÎŸÎÎŸÎœÎ‘"].map(_canon_name)
    class_by_name = dict(zip(df["__CAN_NAME__"], df["Î¤ÎœÎ—ÎœÎ‘"].astype(str).str.strip()))
    friends_by_name = dict(zip(df["__CAN_NAME__"], df[alt_fcol].map(_parse_friends)))

    mutual_pairs = set()
    for a, flist in friends_by_name.items():
        for b in flist:
            if b in friends_by_name and a in friends_by_name[b]:
                mutual_pairs.add(tuple(sorted([a,b])))

    broken_by_class = {tmima: 0 for tmima in df["Î¤ÎœÎ—ÎœÎ‘"].dropna().astype(str).str.strip().unique()}
    for a, b in sorted(mutual_pairs):
        ta = class_by_name.get(a, "")
        tb = class_by_name.get(b, "")
        if ta and tb and ta != tb:
            broken_by_class[ta] = broken_by_class.get(ta, 0) + 1
            broken_by_class[tb] = broken_by_class.get(tb, 0) + 1
    return pd.Series(broken_by_class).fillna(0).astype(int)

def _list_broken_mutual_friendships(df: pd.DataFrame) -> pd.DataFrame:
    """Return DataFrame with each broken mutual pair (A/B with classes)."""
    alt_fcol = "Î¦Î™Î›ÎŸÎ™" if "Î¦Î™Î›ÎŸÎ™" in df.columns else ("Î¦Î™Î›Î™ÎŸÎ™" if "Î¦Î™Î›Î™ÎŸÎ™" in df.columns else None)
    if alt_fcol is None or "ÎŸÎÎŸÎœÎ‘" not in df.columns or "Î¤ÎœÎ—ÎœÎ‘" not in df.columns:
        return pd.DataFrame(columns=["A","A_Î¤ÎœÎ—ÎœÎ‘","B","B_Î¤ÎœÎ—ÎœÎ‘"])

    df = df.copy()
    df["__CAN_NAME__"] = df["ÎŸÎÎŸÎœÎ‘"].map(_canon_name)
    name_to_original = dict(zip(df["__CAN_NAME__"], df["ÎŸÎÎŸÎœÎ‘"].astype(str)))
    class_by_name = dict(zip(df["__CAN_NAME__"], df["Î¤ÎœÎ—ÎœÎ‘"].astype(str).str.strip()))
    friends_by_name = dict(zip(df["__CAN_NAME__"], df[alt_fcol].map(_parse_friends)))

    mutual_pairs = set()
    for a, flist in friends_by_name.items():
        for b in flist:
            if b in friends_by_name and a in friends_by_name[b]:
                mutual_pairs.add(tuple(sorted([a,b])))

    rows = []
    for a, b in sorted(mutual_pairs):
        ta = class_by_name.get(a, "")
        tb = class_by_name.get(b, "")
        if ta and tb and ta != tb:
            rows.append({
                "A": name_to_original.get(a, a), "A_Î¤ÎœÎ—ÎœÎ‘": ta,
                "B": name_to_original.get(b, b), "B_Î¤ÎœÎ—ÎœÎ‘": tb,
            })
    return pd.DataFrame(rows)

